{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Imports and Loading"
      ],
      "metadata": {
        "id": "VwpOcF-1uP2J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQXrqZr-t0yT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Adjust display settings to see all columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads data from csv or xlsx file types.\n",
        "    \"\"\"\n",
        "    if file_path.endswith('.csv'):\n",
        "        return pd.read_csv(file_path)\n",
        "    elif file_path.endswith(('.xls', '.xlsx')):\n",
        "        return pd.read_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type. Please provide a .csv or .xlsx file.\")\n",
        "\n",
        "# === USER INPUT ===\n",
        "file_name = \"your_dataset.csv\"  # REPLACE with actual file name\n",
        "# ==================\n",
        "\n",
        "try:\n",
        "    df = load_data(file_name)\n",
        "    print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Initial Exploration"
      ],
      "metadata": {
        "id": "pbdScZBguXLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- First 5 Rows ---\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\n--- Data Info ---\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\n--- Statistical Summary ---\")\n",
        "# 'include=\"all\"' forces it to summarize strings/dates too, not just numbers\n",
        "display(df.describe(include='all'))"
      ],
      "metadata": {
        "id": "5k-KuWgCudZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Column Name Standardization\n",
        "Inconsistent column names replacement:\n",
        "- Strip whitespace (remove spaces before/after names).\n",
        "- Convert to lower case.\n",
        "- Replace spaces with underscores (snake_case)."
      ],
      "metadata": {
        "id": "SHo0woEsu5Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Columns:\", df.columns.tolist())\n",
        "\n",
        "# Clean columns: strip spaces, lowercase, replace spaces with underscores\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
        "\n",
        "print(\"Cleaned Columns:\", df.columns.tolist())"
      ],
      "metadata": {
        "id": "4iqq9Mi0u0H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Handling Duplicates"
      ],
      "metadata": {
        "id": "xq2Z3a5tvycf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows found: {duplicates}\")\n",
        "\n",
        "print(df.duplicated())"
      ],
      "metadata": {
        "id": "kclYw7Devz9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if duplicates > 0:\n",
        "    df = df.drop_duplicates()\n",
        "    print(\"✅ Duplicates removed.\")\n",
        "else:\n",
        "    print(\"No duplicates to remove.\")"
      ],
      "metadata": {
        "id": "iU9DRnNhv3Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Missing Values"
      ],
      "metadata": {
        "id": "SflPeM1lwH66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Check for missing values\n",
        "missing_percent = df.isnull().mean() * 100\n",
        "print(\"--- Missing Value Percentage by Column ---\")\n",
        "print(missing_percent[missing_percent > 0])\n",
        "\n",
        "# Visualizing missing data\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title(\"Missing Data Heatmap (Yellow = Missing)\")\n",
        "plt.show()\n",
        "\n",
        "# Warning: ACTION REQUIRED BELOW !!!!!\n",
        "\n",
        "# Examples of how to fix specific columns (Uncomment and adapt as needed)\n",
        "\n",
        "# Strategy A: Drop columns with too many missing values (e.g., > 50%)\n",
        "# threshold = 50\n",
        "# cols_to_drop = missing_percent[missing_percent > threshold].index\n",
        "# df = df.drop(columns=cols_to_drop)\n",
        "\n",
        "# Strategy B: Fill Numeric values with Median (safer than mean due to outliers)\n",
        "# num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "# for col in num_cols:\n",
        "#     df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "# Strategy C: Fill Categorical values with 'Unknown'\n",
        "# cat_cols = df.select_dtypes(include=['object']).columns\n",
        "# for col in cat_cols:\n",
        "#     df[col] = df[col].fillna('Unknown')\n",
        "\n",
        "# Re-check\n",
        "print(\"\\nRemaining missing values:\", df.isnull().sum().sum())"
      ],
      "metadata": {
        "id": "ymOf-cHywI-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Data Type Conversion"
      ],
      "metadata": {
        "id": "J8_Ytd24wmvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "jhi1MqSOw-mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Date columns\n",
        "# Identify columns that look like dates (e.g., 'date', 'time', 'created_at')\n",
        "date_cols = [\"aaa\", \"bbb\"]\n",
        "\n",
        "for col in date_cols:\n",
        "    try:\n",
        "        df[col] = pd.to_datetime(df[col])\n",
        "        print(f\"✅ Converted {col} to datetime.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not convert {col}: {e}\")"
      ],
      "metadata": {
        "id": "1k5Qx6Dswn-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean Numeric Strings (e.g., \"$1,200.50\" -> 1200.50)\n",
        "# This regex removes anything that isn't a digit or a decimal point\n",
        "# df['price'] = df['price'].astype(str).str.replace(r'[^\\d.]', '', regex=True)\n",
        "# df['price'] = pd.to_numeric(df['price'])"
      ],
      "metadata": {
        "id": "xDt-7-saxKhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check changes\n",
        "df.info()"
      ],
      "metadata": {
        "id": "M9BQ28DVxSGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: String Cleaning\n",
        "- Strip whitespace.\n",
        "- Standardize capitalization (Title Case or Lowercase)"
      ],
      "metadata": {
        "id": "HfMSZ-VBxZPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all string (object) columns\n",
        "str_cols = df.select_dtypes(include=['object']).columns\n"
      ],
      "metadata": {
        "id": "mvbMA6xCxg7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exclude the 'city' column (and any others you want to protect)\n",
        "# cols_to_exclude = ['city']\n",
        "# str_cols = [col for col in str_cols if col not in cols_to_exclude]"
      ],
      "metadata": {
        "id": "MgmbUHYqypwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in str_cols:\n",
        "    # Strip whitespace from start/end\n",
        "    df[col] = df[col].str.strip()\n",
        "\n",
        "    # Optional: Force logic (Uncomment if needed)\n",
        "    # df[col] = df[col].str.lower()       # Make everything lowercase\n",
        "    # df[col] = df[col].str.title()       # Make everything Title Case\n",
        "\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "id": "puvegmnSx2kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Outlier Detection"
      ],
      "metadata": {
        "id": "9PWvGEkIy5D1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only check numeric columns\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "for col in numeric_cols:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.boxplot(x=df[col])\n",
        "    plt.title(f\"Boxplot of {col}\")\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate bounds\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "    print(f\"Column {col}: {len(outliers)} potential outliers detected.\")"
      ],
      "metadata": {
        "id": "F8CJXjy5y55q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURATION: Choose your method: 'cap', 'drop', or 'none'\n",
        "outlier_method = 'cap'\n",
        "\n",
        "for col in numeric_cols:\n",
        "    # 1. Calculate Bounds\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # 2. Count Outliers\n",
        "    outlier_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
        "    num_outliers = outlier_mask.sum()\n",
        "\n",
        "    if num_outliers > 0:\n",
        "        print(f\"Column '{col}': Found {num_outliers} outliers.\")\n",
        "\n",
        "        # 3. Apply Chosen Method\n",
        "        if outlier_method == 'drop':\n",
        "            df = df[~outlier_mask]\n",
        "            print(f\"Action: Dropped {num_outliers} rows.\")\n",
        "\n",
        "        elif outlier_method == 'cap':\n",
        "            # Replaces values outside bounds with the bounds themselves\n",
        "            df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "            print(f\"Action: Capped values at {lower_bound:.2f} and {upper_bound:.2f}\")\n",
        "\n",
        "        else:\n",
        "            print(\"Action: None (View only)\")\n",
        "    else:\n",
        "        print(f\"Column '{col}': No outliers detected.\")\n",
        "\n",
        "print(f\"\\n Outlier processing complete. Final data shape: {df.shape}\")"
      ],
      "metadata": {
        "id": "3BMjuqakzrHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Logic & Sanity Checks"
      ],
      "metadata": {
        "id": "vG8Svxeo0JTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example A: Check for negative values in columns that must be positive\n",
        "cols_must_be_positive = [] # Add column names here, e.g. ['age', 'price']\n",
        "for col in cols_must_be_positive:\n",
        "    if col in df.columns:\n",
        "        invalid_count = (df[col] < 0).sum()\n",
        "        if invalid_count > 0:\n",
        "            print(f\"Found {invalid_count} negative values in {col}\")\n",
        "            # Fix: e.g., convert to absolute value\n",
        "            # df[col] = df[col].abs()"
      ],
      "metadata": {
        "id": "Onmb5qVd0KSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example B: Date Logic\n",
        "# if 'start_date' in df.columns and 'end_date' in df.columns:\n",
        "#     invalid_dates = df[df['start_date'] > df['end_date']]\n",
        "#     print(f\"Found {len(invalid_dates)} rows where Start Date > End Date\")"
      ],
      "metadata": {
        "id": "ZFO0yM0y0YV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10: Export File"
      ],
      "metadata": {
        "id": "gKA5OFOy0a3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_filename = \"cleaned_data.csv\"\n",
        "df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"Data cleaning complete. Saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "hhzaZ9VA0dVq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}